{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(\"chinni\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0SI3q5UGUg_",
        "outputId": "09030277-e6ff-43f2-d4e6-0b492b162048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chinni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DEBUG ##\n",
        "# !apt-get update\n",
        "# !apt-get install -y build-essential cmake rustc\n",
        "\n",
        "# !pip install -U transformers sentence-transformers\n",
        "\n",
        "# import os\n",
        "# os._exit(00)\n",
        "\n",
        "# import transformers\n",
        "# print(transformers.__version__) # 4.56.1\n",
        "\n",
        "# from transformers import Trainer\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# try:\n",
        "#     # Initialize the SentenceTransformer model\n",
        "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "#     print(\"Model loaded successfully!\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bOoo_qwKM4jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT_gGVbhKxS2",
        "outputId": "b1ce7198-e2fd-41ee-ffd7-ef5e9a99297d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community\n",
        "!pip install  pypdf pymupdf tqdm\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu chromadb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w149kDxKDLH",
        "outputId": "06297188-ee19-41ff-b51f-909c7b37ab71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.75)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "\u001b[2K  Attempting uninstall: requests\n",
            "\u001b[2K    Found existing installation: requests 2.32.4\n",
            "\u001b[2K    Uninstalling requests-2.32.4:\n",
            "\u001b[2K      Successfully uninstalled requests-2.32.4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [langchain-community]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, pymupdf\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pymupdf]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pymupdf-1.26.4 pypdf-6.0.0\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.21-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.21-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m138.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "Downloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=3edd97dabbf10a10db859e61721003871afb681314abe13ca36e7349f891f44b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, faiss-cpu, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "\u001b[2K  Attempting uninstall: opentelemetry-api\n",
            "\u001b[2K    Found existing installation: opentelemetry-api 1.36.0\n",
            "\u001b[2K    Uninstalling opentelemetry-api-1.36.0:\n",
            "\u001b[2K      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "\u001b[2K  Attempting uninstall: opentelemetry-sdk\n",
            "\u001b[2K    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "\u001b[2K    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [chromadb]\n",
            "\u001b[1A\u001b[2KSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.21 coloredlogs-15.0.1 durationpy-0.10 faiss-cpu-1.12.0 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Python"
      ],
      "metadata": {
        "id": "oEw5pUo0JdQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "Fq9x2FuPZx7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document"
      ],
      "metadata": {
        "id": "rVNPgcjnJZ2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "4xIUIWjRJP89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Document(\n",
        "    page_content=\"Hello, Chinni!, This is your first set-up for RAG, Please do more!!!!\",\n",
        "    metadata=\n",
        "    {\"source\": \"example.txt\",\n",
        "     \"author\": \"Chinni Krishna\",\n",
        "     \"page\": 1,\n",
        "     \"status\": \"active\",\n",
        "     \"date_created\": \"2025-06-06\",\n",
        "     \"version\": \"1.0\"},\n",
        "    additional_info={\"notes\": \"This is a sample document for testing.\"},\n",
        "    tags=[\"sample\", \"test\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "ExpYFS5LJRN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS62w-E_JT6q",
        "outputId": "10ebf6c6-41c1-42e0-f4e4-a58f2e043248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'example.txt', 'author': 'Chinni Krishna', 'page': 1, 'status': 'active', 'date_created': '2025-06-06', 'version': '1.0'}, page_content='Hello, Chinni!, This is your first set-up for RAG, Please do more!!!!')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85wufGqfJWOm",
        "outputId": "7af3d166-d82e-43bc-c083-3185059265db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('data/text_files', exist_ok=True)"
      ],
      "metadata": {
        "id": "2f_TE0A1JlOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = {\"data/text_files/abc.txt\": \"This is my first sample text file\", \"data/text_files/mno.txt\": \"This is a sample text file.\", \"data/text_files/xyz.txt\": \"This is another sample text file.\"}\n",
        "\n",
        "for file_path, content in sample_text.items():\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "        file.close()\n",
        "\n",
        "\n",
        "print(\"Files have been created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6-4HbauJniv",
        "outputId": "c1f59f33-d840-4fb7-e9f1-f7a1d509e378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADER"
      ],
      "metadata": {
        "id": "Ov8O4m8hJv3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Loader"
      ],
      "metadata": {
        "id": "XPhEpi9-J3Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Single file Loader\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(f'data/text_files/abc.txt', encoding='utf-8')\n",
        "\n",
        "data1 = loader.load()\n",
        "print(data1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM6mpibkJ6YG",
        "outputId": "46e9f165-fde1-49c1-ece2-7145c9383fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'data/text_files/abc.txt'}, page_content='This is my first sample text file')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Multi-file Loader\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "dir_loader = DirectoryLoader(\n",
        "    'data/text_files',\n",
        "    glob='*.txt',\n",
        "    loader_cls=TextLoader,\n",
        "    loader_kwargs={'encoding': 'utf-8'},\n",
        "    show_progress=True\n",
        "    )\n",
        "data2 = dir_loader.load()\n",
        "print(data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPu7zeIfJ0vg",
        "outputId": "052249c9-d1e8-4f3f-c890-b084244b147b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 3707.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'data/text_files/xyz.txt'}, page_content='This is another sample text file.'), Document(metadata={'source': 'data/text_files/abc.txt'}, page_content='This is my first sample text file'), Document(metadata={'source': 'data/text_files/mno.txt'}, page_content='This is a sample text file.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Loader"
      ],
      "metadata": {
        "id": "6tUFxY5cLlE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "\n",
        "dir_loader = DirectoryLoader(\n",
        "    'data/pdf',\n",
        "    glob='**/*.pdf',\n",
        "    loader_cls=PyMuPDFLoader,\n",
        "    show_progress=True\n",
        "    )\n",
        "pdf_documents = dir_loader.load()\n",
        "print(pdf_documents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "rG7hveSZLjZs",
        "outputId": "105fa527-bbb5-440b-bd7d-00a86925eae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Directory not found: 'data/pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4260070158.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpdf_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/document_loaders/directory.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;34m\"\"\"Load documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/document_loaders/directory.py\u001b[0m in \u001b[0;36mlazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Directory not found: '{self.path}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected directory, got file: '{self.path}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Directory not found: 'data/pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pdf_documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "rxslm9dYL8Ol",
        "outputId": "e6531a6f-6c6b-4805-b73d-89d58fda7571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 256);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "2teclwlPdLRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Read all the pdf's inside the directory\n",
        "def process_all_pdfs(pdf_directory):\n",
        "    \"\"\"Process all PDF files in a directory\"\"\"\n",
        "    all_documents = []\n",
        "    pdf_dir = Path(pdf_directory)\n",
        "\n",
        "    # Find all PDF files recursively\n",
        "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        try:\n",
        "            loader = PyPDFLoader(str(pdf_file))\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Add source information to metadata\n",
        "            for doc in documents:\n",
        "                doc.metadata['source_file'] = pdf_file.name\n",
        "                doc.metadata['file_type'] = 'pdf'\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
        "    return all_documents\n",
        "\n",
        "# Process all PDFs in the data directory\n",
        "all_pdf_documents = process_all_pdfs(\"data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ZxERE1dODl",
        "outputId": "e6669662-7629-4c0a-e628-7017e520e626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 PDF files to process\n",
            "\n",
            "Processing: chinni-resume.pdf\n",
            "  ✓ Loaded 2 pages\n",
            "\n",
            "Total documents loaded: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pdf_documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "OYoGUDoedbdK",
        "outputId": "20932c5e-c727-4fef-d7b3-158ae4ea98b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_pdf_documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2281664588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_pdf_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'all_pdf_documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunks"
      ],
      "metadata": {
        "id": "bj8L1Qp7eW-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Text splitting get into chunks\n",
        "\n",
        "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
        "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
        "\n",
        "    # Show example of a chunk\n",
        "    if split_docs:\n",
        "        print(f\"\\nExample chunk:\")\n",
        "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
        "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
        "\n",
        "    return split_docs"
      ],
      "metadata": {
        "id": "LN1Q5iCkd5P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=split_documents(all_pdf_documents)\n",
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "DDA0WYend-IK",
        "outputId": "07ede1bd-ebdb-4987-801e-2a3ca02ab1da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'split_documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1863053599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pdf_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'split_documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding & VectorStoreDB"
      ],
      "metadata": {
        "id": "XZUCbMc5L_YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "O_SnJ1gFN5lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Optional, Tuple, Callable, Iterable, Type, Iterator\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "59g7VmmgL-VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize the embedding manager\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model name for sentence embeddings\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading embedding model: {self.model_name}\")\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for a list of texts\n",
        "\n",
        "        Args:\n",
        "            texts: List of text strings to embed\n",
        "\n",
        "        Returns:\n",
        "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
        "        \"\"\"\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "\n",
        "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "## initialize the embedding manager\n",
        "\n",
        "embedding_manager=EmbeddingManager()\n",
        "embedding_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqt6K1l8MZtU",
        "outputId": "793b9a93-d3d8-4274-8b29-30b59024b65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2\n",
            "Model loaded successfully. Embedding dimension: 384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EmbeddingManager at 0x7ca8d18cd3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  VectorStore"
      ],
      "metadata": {
        "id": "0EGn50ckN0WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
        "\n",
        "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
        "        \"\"\"\n",
        "        Initialize the vector store\n",
        "\n",
        "        Args:\n",
        "            collection_name: Name of the ChromaDB collection\n",
        "            persist_directory: Directory to persist the vector store\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self._initialize_store()\n",
        "\n",
        "    def _initialize_store(self):\n",
        "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
        "        try:\n",
        "            # Create persistent ChromaDB client\n",
        "            os.makedirs(self.persist_directory, exist_ok=True)\n",
        "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "            # Get or create collection\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
        "            )\n",
        "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
        "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
        "        \"\"\"\n",
        "        Add documents and their embeddings to the vector store\n",
        "\n",
        "        Args:\n",
        "            documents: List of LangChain documents\n",
        "            embeddings: Corresponding embeddings for the documents\n",
        "        \"\"\"\n",
        "        if len(documents) != len(embeddings):\n",
        "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
        "\n",
        "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
        "\n",
        "        # Prepare data for ChromaDB\n",
        "        ids = []\n",
        "        metadatas = []\n",
        "        documents_text = []\n",
        "        embeddings_list = []\n",
        "\n",
        "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
        "            # Generate unique ID\n",
        "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "            ids.append(doc_id)\n",
        "\n",
        "            # Prepare metadata\n",
        "            metadata = dict(doc.metadata)\n",
        "            metadata['doc_index'] = i\n",
        "            metadata['content_length'] = len(doc.page_content)\n",
        "            metadatas.append(metadata)\n",
        "\n",
        "            # Document content\n",
        "            documents_text.append(doc.page_content)\n",
        "\n",
        "            # Embedding\n",
        "            embeddings_list.append(embedding.tolist())\n",
        "\n",
        "        # Add to collection\n",
        "        try:\n",
        "            self.collection.add(\n",
        "                ids=ids,\n",
        "                embeddings=embeddings_list,\n",
        "                metadatas=metadatas,\n",
        "                documents=documents_text\n",
        "            )\n",
        "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
        "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding documents to vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "vectorstore=VectorStore()\n",
        "vectorstore\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxpMabyLM-4w",
        "outputId": "062171bf-7b57-4a9b-b8ea-b9b2e4a383a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store initialized. Collection: pdf_documents\n",
            "Existing documents in collection: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.VectorStore at 0x7ca8d18cf740>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWbLbdyFf9_e",
        "outputId": "41b0cb7d-708f-4224-9e02-667f419723ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vAFV71LgfoQE",
        "outputId": "ae1e017d-897d-4c9b-84f8-1b0c734bc0fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-45558693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Convert the text to embeddings\n",
        "texts=[doc.page_content for doc in chunks]\n",
        "\n",
        "## Generate the Embeddings\n",
        "\n",
        "embeddings=embedding_manager.generate_embeddings(texts)\n",
        "\n",
        "##store in the vector dtaabase\n",
        "vectorstore.add_documents(chunks,embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "2c43c26274434166a3a691d4461c5d99",
            "5cdc69c2b85d46ccba8a8d6cbe355141",
            "4c4be1f56ff34b36af8c4c9e31cd8d43",
            "4820734b629940c98db08a46a59f436c",
            "a2da08d0e66e40dc834abd6956847cfa",
            "2f96c5805ed14b75bb715135b75f89b8",
            "403046370a62445fa861ee58b4c75f80",
            "854b5265abaf4c808ba5f686d24edb18",
            "44563e76231946c98576bccba4d3d3e0",
            "5eaa038bcef74d6e8dd2775d37345dc3",
            "75589c7b3e52460193d20246210feb51"
          ]
        },
        "id": "liPWfh3hgEAS",
        "outputId": "c7f2e316-6474-49ce-e5c9-21b2b3488667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for 11 texts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c43c26274434166a3a691d4461c5d99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (11, 384)\n",
            "Adding 11 documents to vector store...\n",
            "Successfully added 11 documents to vector store\n",
            "Total documents in collection: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Retriever Pipeline from VectorStore"
      ],
      "metadata": {
        "id": "xZaR2QwEg77d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGRetriever:\n",
        "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
        "        \"\"\"\n",
        "        Initialize the retriever\n",
        "\n",
        "        Args:\n",
        "            vector_store: Vector store containing document embeddings\n",
        "            embedding_manager: Manager for generating query embeddings\n",
        "        \"\"\"\n",
        "        self.vector_store = vector_store\n",
        "        self.embedding_manager = embedding_manager\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant documents for a query\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "            top_k: Number of top results to return\n",
        "            score_threshold: Minimum similarity score threshold\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing retrieved documents and metadata\n",
        "        \"\"\"\n",
        "        print(f\"Retrieving documents for query: '{query}'\")\n",
        "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
        "\n",
        "        # Search in vector store\n",
        "        try:\n",
        "            results = self.vector_store.collection.query(\n",
        "                query_embeddings=[query_embedding.tolist()],\n",
        "                n_results=top_k\n",
        "            )\n",
        "\n",
        "            # Process results\n",
        "            retrieved_docs = []\n",
        "\n",
        "            if results['documents'] and results['documents'][0]:\n",
        "                documents = results['documents'][0]\n",
        "                metadatas = results['metadatas'][0]\n",
        "                distances = results['distances'][0]\n",
        "                ids = results['ids'][0]\n",
        "\n",
        "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
        "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
        "                    similarity_score = 1 - distance\n",
        "\n",
        "                    if similarity_score >= score_threshold:\n",
        "                        retrieved_docs.append({\n",
        "                            'id': doc_id,\n",
        "                            'content': document,\n",
        "                            'metadata': metadata,\n",
        "                            'similarity_score': similarity_score,\n",
        "                            'distance': distance,\n",
        "                            'rank': i + 1\n",
        "                        })\n",
        "\n",
        "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
        "            else:\n",
        "                print(\"No documents found\")\n",
        "\n",
        "            return retrieved_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
      ],
      "metadata": {
        "id": "H__zUD1HhEoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC1cCZJahEVw",
        "outputId": "40be7f1f-1905-4bc5-d472-75918e68013d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RAGRetriever at 0x7ca8d1d97590>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_retriever.retrieve(\"what do you know about company?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "MF0wjgkIhQgZ",
        "outputId": "4f69ce85-3ebf-418a-fb04-c5630cf53c22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag_retriever' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2022553086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrag_retriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what do you know about Pantech Solutions?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rag_retriever' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline - VectorDB To LLM Output Generation"
      ],
      "metadata": {
        "id": "L91aUYf1jdQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHATGPT_API_KEY = \"\""
      ],
      "metadata": {
        "id": "gmV70vv5jOOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=CHATGPT_API_KEY)  # Replace with your actual key\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ztQS5p8YsICC",
        "outputId": "2e272038-4304-4de4-8b67-dd51697b7024"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CHATGPT_API_KEY' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-584967303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHATGPT_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your actual key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m response = client.chat.completions.create(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CHATGPT_API_KEY' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from typing import Optional"
      ],
      "metadata": {
        "id": "SdAdHoA3jpiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenAILLM:\n",
        "    def __init__(self, model_name: str = \"gpt-4\", api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize OpenAI ChatGPT LLM\n",
        "\n",
        "        Args:\n",
        "            model_name: OpenAI model name (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n",
        "            api_key: OpenAI API key (or set OPENAI_API_KEY environment variable)\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.api_key = CHATGPT_API_KEY\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass api_key parameter.\")\n",
        "\n",
        "        self.llm = ChatOpenAI(\n",
        "            openai_api_key=self.api_key,\n",
        "            model_name=self.model_name,\n",
        "            temperature=0.1,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "\n",
        "        print(f\"Initialized OpenAI LLM with model: {self.model_name}\")\n",
        "\n",
        "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
        "        \"\"\"\n",
        "        Generate response using retrieved context\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            context: Retrieved document context\n",
        "            max_length: Maximum response length\n",
        "\n",
        "        Returns:\n",
        "            Generated response string\n",
        "        \"\"\"\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "            template=\"\"\"You are a helpful AI assistant for Chinni Krishna. Use the following context to answer the question accurately and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
        "        )\n",
        "\n",
        "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
        "\n",
        "        try:\n",
        "            messages = [HumanMessage(content=formatted_prompt)]\n",
        "            response = self.llm.invoke(messages)\n",
        "            return response.content\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def generate_response_simple(self, query: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Simple response generation without complex prompting\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            context: Retrieved context\n",
        "\n",
        "        Returns:\n",
        "            Generated response\n",
        "        \"\"\"\n",
        "        simple_prompt = f\"\"\"Based on this context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            messages = [HumanMessage(content=simple_prompt)]\n",
        "            response = self.llm.invoke(messages)\n",
        "            return response.content\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "zDVxMx0qj4Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    openai_llm = OpenAILLM(api_key=CHATGPT_API_KEY)\n",
        "    print(\"OPENAI LLM initialized successfully!\")\n",
        "except ValueError as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    print(\"Please set your OPENAI KEY environment variable to use the LLM.\")\n",
        "    openai_llm = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1LzPBDpleVe",
        "outputId": "2c3a7ac8-bb37-44e9-cf4b-eaa6fb379263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized OpenAI LLM with model: gpt-4\n",
            "OPENAI LLM initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### get the context from the retriever and pass it to the LLM\n",
        "\n",
        "rag_retriever.retrieve(\"DevOps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "bF5MWge1mDZl",
        "outputId": "573683ba-e46e-454b-eac4-6b8d1b9172ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag_retriever' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4070933518.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### get the context from the retriever and pass it to the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrag_retriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DevOps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rag_retriever' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "### Initialize the OpenAI LLM (set your OPENAI_API_KEY in environment)\n",
        "openai_api_key = CHATGPT_API_KEY\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=openai_api_key,\n",
        "    model_name=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
        "    temperature=0.1,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "## 2. Simple RAG function: retrieve context + generate response\n",
        "def rag_simple(query, retriever, llm, top_k=3):\n",
        "    # Retrieve the context\n",
        "    results = retriever.retrieve(query, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
        "\n",
        "    if not context:\n",
        "        return \"No relevant context found to answer the question.\"\n",
        "\n",
        "    # Generate the answer using OpenAI LLM\n",
        "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content\n"
      ],
      "metadata": {
        "id": "8x_Sl3JCmlNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=rag_simple(\"DevOps\",rag_retriever,llm)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "hEZgPn9NoolI",
        "outputId": "03116637-5382-4e63-d5d9-c45ccfbfbe6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag_simple' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2414973546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrag_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DevOps\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrag_retriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rag_simple' is not defined"
          ]
        }
      ]
    }
  ]
}